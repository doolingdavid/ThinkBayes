{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series, DataFrame\n",
    "import pytz\n",
    "from pytz import common_timezones, all_timezones\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think Bayes: Bayesian Statistics Made Simple\n",
    "\n",
    "There are several excellennt modules for doing Bayesian statistics in Python, incluidng ```pymc``` and ```OpenBUGS```.\n",
    "I chose not to use them for this book because you need a fair amount of background knowledge to get started with these modules, and I want to keep the prerequisites minimal. If you know Python and a little bit about probability, you are ready to start this book.\n",
    "\n",
    "\n",
    "Chapter 1 is about probability and Baye's theorem; it has no code. Chapter 2 introduces ```Pmf```, a thinly disguised Python dictionary I use to represent a probablity mass function (PMF). Then Chapter 3 introduces ```Suite```, a kind of Pmf that provides a framework for doing Bayesian updates. And that's just about all there is to it.\n",
    "\n",
    "Well, almost. In some of the later chapters, I use analytic distributions including the Guassian (normal) distribution, the exponential and Poisson distributions, and the beta distribution. In Chapter 15 I break out the less-common Dirichlet distribution, but I explain it as I go along. If you are not familiar with these distributions, you can read about them on Wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025241157556270097"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "785000./311000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual notation for conditional probability is \n",
    "$p(A|B)$, which is the probablity of $A$ given that $B$ is true. In this example, $A$ represents the prediction that I will have a heart attack in teh next year, and $B$ is the set of conditions I listed.\n",
    "\n",
    "### Conjoint probability\n",
    "\n",
    "**Conjoint probability** is a fancy way to say the probability that two things are true. I write $p(A \\mbox{and} B)$ to mean the probability that $A$ and $B$ are both true.\n",
    "In geneneral, the probability of a conjunction is\n",
    "\n",
    "$\n",
    "p(A \\mbox{and} B) = p(A) p(B|A)\n",
    "$\n",
    "\n",
    "for any $A$ and $B$. So if the chance of rain on any given day is 0.5, the chance of rain on two consecutive days is not 0.25, but probably a bit higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cookie problem\n",
    "\n",
    "We'll get to Bayes's theorem soon, but I wanted to motivate it with an example called the cookie problem. Suppose there are two bowls of cookies. Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies. Bowl 2 contains 20 of each. Now suppose you choose one of the bowls at random and, without looking, select a cookie at ranom. The cookie is vanilla. What is the probability that it came from Bowl 1?\n",
    "\n",
    "This is a conditional probability; we want $p(\\mbox{Bowl 1} | \\mbox{vanilla})$, but it is not obvious how to compute it. If I asked a different question - the probability of a vanilla cookie given Bowl 1 - it would be easy:\n",
    "\n",
    "$\n",
    "p(\\mbox{vanilla} | \\mbox{Bowl 1}) = \\frac{30}{40}\n",
    "$\n",
    "\n",
    "Sadly, $p(A|B)$ is _not_ the same as $p(B|A)$, but there is a way to get from one to the other: Bayes's theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes's theorem\n",
    "\n",
    "At this point we have everything we need to derive Bayes's theorem. We'll start with the observation that conjunction is commutataive; that is\n",
    "\n",
    "$\n",
    "p(A \\mbox{and} B) = p(B \\mbox{and} A)\n",
    "$\n",
    "\n",
    "Next, we write the probability of a conjunction:\n",
    "\n",
    "$\n",
    "p(A \\mbox{and} B) = p(A)p(B|A)\n",
    "$\n",
    "\n",
    "Since we have not said anything about what $A$ and $B$ mean, they are interchangeable. Interchanging them yields\n",
    "\n",
    "$\n",
    "p(B \\mbox{and} A) = p(B)p(A|B)\n",
    "$\n",
    "\n",
    "That's all we need. Pulling these pieces together, we get\n",
    "\n",
    "$\n",
    "p(B)p(A|B) = p(A)p(B|A)\n",
    "$\n",
    "\n",
    "which means there are two ways to compute the conjunction. If you have $p(A)$, you multiply by the conditional probability\n",
    "$p(B|A)$. Or you can do it the other way around: if you know $p(B)$, you multiply by $p(A|B)$. Either way you should get the same thing. \n",
    "Finally we can divide through by $p(B)$:\n",
    "\n",
    "$\n",
    "p(A|B)  = \\frac{p(A) p(B|A)}{p(B)}\n",
    "$\n",
    "\n",
    "And that's Bayes's theorem! It might not look like much, but it turns out to be suprisingly powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can use it to solve the cookie problem. I'll write $B_1$ for the hypothesis that the cookie came from Bowl 1 and $V$ for the vanilla cookie. Plugging into Bayes's theorem we get\n",
    "\n",
    "$\n",
    "p(B_{1} | V) = \\frac{p(B_{1})p(V | B_{1})}{p(V)}\n",
    "$\n",
    "\n",
    "The term on the left is what we want: the probability of Bowl 1, given that we chose a vanilla cookie. the terms on the right are:\n",
    "\n",
    "* $p(B_1)$: This is the probability that we chose Bowl 1, unconditioned by what kind of cookie we got. Since the problem says we chose a bowl at random, we can assume $p(B_1) = \\frac{1}{2}$.\n",
    "* $p(V|B_1)$: This is the probability of getting a vanilla cookie from Bowl 1, which is $\\frac{3}{4}$.\n",
    "* $p(V)$: This is the probability of drawing a vanilla cookie from either bowl. Since we had an equal change of choosing either bowl and the bowls contain the same number of cookies, we had the same chance of choosing any cookie. use $p(B_1)p(V|B_1) + p(B_2)p(V|B_2) = \\frac{5}{8}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The diachronic interpretation\n",
    "\n",
    "There is another way to think of Bayes's theorem: it gives us a way to update the probability of a hypothesis, $H$, in light of some body of data, $D$.\n",
    "\n",
    "This way of thinking about Baye's theorem is called the **diachronic interpretation**. \"Diachronic\" means that something is happening over time; in this case the probability of the hypothesis changtes, over time, as we see new data.\n",
    "\n",
    "Rewriting Bayes's theorem with $H$ and $D$ yields:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "p(H|D) = \\frac{p(H) p(D|H)}{p(D)}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "In this interpretatoin, each term has a name:\n",
    "\n",
    "* $p(H)$ is the probability of the hypothesis before we see the data, called the prior probability, or just **prior**\n",
    "* $p(H|D)$ is what we want to compute, the probability of the hypothesis after we see the data, called the **posterior**.\n",
    "* $p(D|H)$ is the probablity of the data under the hypothesis, called the **likelihood**.\n",
    "* $p(D)$ is the probability of the data under any hypothesis, called the **normalizing constant**.\n",
    "\n",
    "Sometimes we can compute the prior based on background information. For example, the cookie problem specifies that we choose a bowl at random with equal probability. In other cases the prior is subjective; that is, reasonable people might disagree, either because they use different background information or because they interpret the same information differently.\n",
    "The likelihood is usually the easiest part to compute. In the cookie problem, if we know which bowl the cookie came from, we find the probability of a vanilla cookie by counting.\n",
    "\n",
    "The normalizing constant can be tricky. It is supposed to be the probability of seeing the data under any hypothesis at all, but in the most general case it is hard to nail down what that means.\n",
    "\n",
    "Most often we simplify things by specifiying a set of hypothesis that are\n",
    "\n",
    "* **Mutually exclusive**: At most one hypothesis in the set can be true, and\n",
    "* **Collectively exhaustive**: There are no other possiblities; at least one of the hypotheses has to be true.\n",
    "\n",
    "I use the word **suite** for a set of hypothesis that has these properties.\n",
    "\n",
    "In the cookie problem, there are only two hypotheses - the cookie came from Bowl 1 or Bowl 2 - and they are mutually exclusive and collectively exhaustive. In that case we can compute $p(D)$ using the law of total probability, which says that if there are two exclusive ways that something might happen, you can add up the probabilities like this:\n",
    "\n",
    "$\n",
    "p(D) = p(B_1)p(D|B_1) + p(B_2)p(D|B_2)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The M&M problem\n",
    "\n",
    "M&M's are small candy-coated chocolates that come in a variety of colors. Mars, Inc., which makes M&M's, changes the mixture of colors from time to time.\n",
    "\n",
    "In 1995, they introduced blue M&M's. Before then, the color mix in a bag of plain M&M's was 30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Tan. Afterward it was 24% Blue, 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown.\n",
    "\n",
    "Suppose a friend of mine has two bags of M&M's, and he tells me that one is from 1994 and one from 1996. He wont tell me which is which, but he gives me one M&M from each bag. One is yellow and one is green. What is the probability that the yellow one came from the 1994 bag?\n",
    "\n",
    "\n",
    "This problem is similar to the cookie problem, with the twist that I draw one samle form each bowl/bag. This problem also gives me a chance to demonstrate the table method, which is useful for solving problems like this on paper. In the next chapter we will solve them computationally.\n",
    "\n",
    "The first step is to enumate the hypothesis. The bag the yellow M&M came from I'll call Bag 1; I'll call the other Bag 2.\n",
    "So the hypotheses are:\n",
    "\n",
    "* A: Bag 1 is from 1994, which implies that Bag 2 is from 1996.\n",
    "* B: Bag 1 is from 1996 and Bag 2 from 1994.\n",
    "\n",
    "\n",
    "Now we construct a table with a row for each hypothesis and a column for each term in Baye's theorem:\n",
    "\n",
    "\n",
    "|   | Prior | Likelihood | | Posterior |\n",
    "-------------| :-----------: | :------------: | :------------: | :-------------: |\n",
    "|  | $p(H)$ | $p(D|H)$  |   $p(H)p(D|H)$ | $p(H|D)$ |\n",
    "| A | 1/2 |  (20)(20)    | 200 |    20/27 | \n",
    "| B | 1/2 | (14)(10)  |     70 | 7/27 | \n",
    "\n",
    "\n",
    "\n",
    "The first column has the priors. Based on the statement of the problem, it is reasonable to choose $p(A) = p(B) = 1/2$.\n",
    "\n",
    "The second column has the likelihoods, which follow from the infomration in the problem. For example, if $A$ is true, the yellow M&M came from the 1994 bag with probability 20%, and the green came from the 1996 bag with probability 20%. If B is true, the yellow M&M came from the 1996 bag with probability 14%, and the green came from teh 1994 bag with probabilty 10%. Because the selections are independent, we get the conjoint probability by multiplying.\n",
    "\n",
    "The third column is just he product of the previous two. The sum of this column, 270, is the normalizing constant. To get the last column, which contains the posteriors, we divide the third column by the normalizing constant.\n",
    "\n",
    "That's it. Simple, right?\n",
    "\n",
    "Well, you might be bothered by one detail. I write $p(D|H)$ in terms of percentages, not probabilities, which means it is off by a factor of 10,000. But that cancels out when we divide through by the normalizing constant, so it doesn't affect the result.\n",
    "\n",
    "When the set of hypotheses is mutually exlusive and collectively exhaustive, you can multiply the likelihoods by any factor, if it is convenient, as long as you apply the same factor to the entire column.\n",
    "\n",
    "\n",
    "```\n",
    "|             |          Grouping           ||\n",
    "First Header  | Second Header | Third Header |\n",
    " ------------ | :-----------: | -----------: |\n",
    "Content       |          *Long Cell*        ||\n",
    "Content       |   **Cell**    |         Cell |\n",
    "\n",
    "New section   |     More      |         Data |\n",
    "And more      | With an escaped '\\|'         ||  \n",
    "[Prototype table]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
