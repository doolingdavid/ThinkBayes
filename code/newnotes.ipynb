{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series, DataFrame\n",
    "import pytz\n",
    "from pytz import common_timezones, all_timezones\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://greenteapress.com/wp/think-bayes/\n",
    "\n",
    "Roger Labbe has transformed _Think Bayes_ into IPython notebooks where you can modify and run the code\n",
    "\n",
    "There are several excellent modules for doing Bayesian statistics in Python, including pymc and OpenBUGS. I chose not to use them for this book because you need a fair amount of background knowledge to get started with these modules, and I want to keep the prerequisites minimal. If you know Python and a little bit about probability, you are ready to start this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 Bayes's Theorem\n",
    "\n",
    "### Conditional probability\n",
    "\n",
    "The usual notation for conditional probability is p(A|B), which is the probability of A given that B is true.\n",
    "\n",
    "### Conjoint probability\n",
    "\n",
    "In general, the probability of a conjunction is\n",
    "\n",
    "p(A  and  B) = p(A) p(B|A) \n",
    "\n",
    "### The cookie problem\n",
    "\n",
    "We’ll get to Bayes’s theorem soon, but I want to motivate it with an example called the cookie problem.1 Suppose there are two bowls of cookies. Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies. Bowl 2 contains 20 of each.\n",
    "Now suppose you choose one of the bowls at random and, without looking, select a cookie at random. The cookie is vanilla. What is the probability that it came from Bowl 1?\n",
    "\n",
    "This is a conditional probability; we want p(Bowl 1 | vanilla), but it is not obvious how to compute it. If I asked a different question—the probability of a vanilla cookie given Bowl 1—it would be easy:\n",
    "\n",
    "p(vanilla | Bowl 1) = 3/4 \n",
    "Sadly, p(A|B) is not the same as p(B|A), but there is a way to get from one to the other: Bayes’s theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes's theorem\n",
    "\n",
    "$\n",
    "p(A|B) = \\frac{ p(A) p(B|A)}{p(B)}\n",
    "$ \n",
    "\n",
    "And that’s Bayes’s theorem! It might not look like much, but it turns out to be surprisingly powerful.\n",
    "\n",
    "$\n",
    "p(B_1|V) = \\frac{(1/2) (3/4)}{ (1/2)(3/4) + (1/2)(1/2)} = 3/5\n",
    "$\n",
    "\n",
    "So the vanilla cookie is evidence in favor of the hypothesis that we chose Bowl 1, because vanilla cookies are more likely to come from Bowl 1.\n",
    "\n",
    "This example demonstrates one use of Bayes’s theorem: it provides a strategy to get from p(B|A) to p(A|B). This strategy is useful in cases, like the cookie problem, where it is easier to compute the terms on the right side of Bayes’s theorem than the term on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The diachronic interpretation\n",
    "\n",
    "There is another way to think of Bayes’s theorem: it gives us a way to update the probability of a hypothesis, H, in light of some body of data, D.\n",
    "This way of thinking about Bayes’s theorem is called the diachronic interpretation. “Diachronic” means that something is happening over time; in this case the probability of the hypotheses changes, over time, as we see new data.\n",
    "\n",
    "Rewriting Bayes’s theorem with H and D yields:\n",
    "\n",
    "$\n",
    "p(H|D) = \\frac{p(H)p(D|H)}{p(D)}\n",
    "$\n",
    "\n",
    "In this interpretation, each term has a name:\n",
    "\n",
    "* p(H) is the probability of the hypothesis before we see the data, called the prior probability, or just **prior**.\n",
    "* p(H|D) is what we want to compute, the probability of the hypothesis after we see the data, called the **posterior**.\n",
    "* p(D|H) is the probability of the data under the hypothesis, called the **likelihood**.\n",
    "* p(D) is the probability of the data under any hypothesis, called the **normalizing constant**.\n",
    "\n",
    "Sometimes we can compute the prior based on background information. For example, the cookie problem specifies that we choose a bowl at random with equal probability.\n",
    "In other cases the prior is subjective; that is, reasonable people might disagree, either because they use different background information or because they interpret the same information differently.\n",
    "\n",
    "The likelihood is usually the easiest part to compute. In the cookie problem, if we know which bowl the cookie came from, we find the probability of a vanilla cookie by counting.\n",
    "\n",
    "The normalizing constant can be tricky. It is supposed to be the probability of seeing the data under any hypothesis at all, but in the most general case it is hard to nail down what that means.\n",
    "\n",
    "Most often we simplify things by specifying a set of hypotheses that are\n",
    "\n",
    "**Mutually exclusive**:\n",
    "At most one hypothesis in the set can be true, and\n",
    "**Collectively exhaustive**:\n",
    "There are no other possibilities; at least one of the hypotheses has to be true.\n",
    "I use the word suite for a set of hypotheses that has these properties.\n",
    "\n",
    "\n",
    "In the cookie problem, there are only two hypotheses—the cookie came from Bowl 1 or Bowl 2—and they are mutually exclusive and collectively exhaustive.\n",
    "\n",
    "In that case we can compute p(D) using the law of total probability, which says that if there are two exclusive ways that something might happen, you can add up the probabilities like this:\n",
    "\n",
    "$\n",
    "p(D) = p(B1) p(D|B1) + p(B2) p(D|B2) \n",
    "$\n",
    "\n",
    "\n",
    "Plugging in the values from the cookie problem, we have\n",
    "\n",
    "\n",
    "$\n",
    "p(D) = (1/2) (3/4) + (1/2) (1/2) = 5/8 \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The M&M problem\n",
    "\n",
    "M&M’s are small candy-coated chocolates that come in a variety of colors. Mars, Inc., which makes M&M’s, changes the mixture of colors from time to time.\n",
    "In 1995, they introduced blue M&M’s. Before then, the color mix in a bag of plain M&M’s was 30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan. Afterward it was 24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown.\n",
    "\n",
    "Suppose a friend of mine has two bags of M&M’s, and he tells me that one is from 1994 and one from 1996. He won’t tell me which is which, but he gives me one M&M from each bag. One is yellow and one is green. What is the probability that the yellow one came from the 1994 bag?\n",
    "\n",
    "This problem is similar to the cookie problem, with the twist that I draw one sample from each bowl/bag. This problem also gives me a chance to demonstrate the table method, which is useful for solving problems like this on paper. In the next chapter we will solve them computationally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to enumerate the hypotheses. The bag the yellow M&M came from I’ll call Bag 1; I’ll call the other Bag 2. So the hypotheses are:\n",
    "\n",
    "* A: Bag 1 is from 1994, which implies that Bag 2 is from 1996.\n",
    "* B: Bag 1 is from 1996 and Bag 2 is from 1994.\n",
    "\n",
    "Now we construct a table with a row for each hypothesis and a column for each term in Bayes's theorem:\n",
    " \n",
    "| | p(H) | p(D &#124; H)  | p(H)p(D &#124; H) | p(H &#124; D) | \n",
    "|:-----:|:--------------:|:-----------------:|:-------------:|:---------------------:|\n",
    "|A| 1/2 | (20)(20) | 200 |  20/27     |\n",
    "|B| 1/2 | (14)(10)  | 70   |   7/27 |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second column has the likelihoods, which follow from the information in the problem. For example, if A is true, the yellow M&M came from the 1994 bag with probability 20%, and the green came from the 1996 bag with probability 20%. If B is true, the yellow M&M came from the 1996 bag with probability 14%, and the green came from the 1994 bag with probability 10%. Because the selections are independent, we get the conjoint probability by multiplying.\n",
    "\n",
    "\n",
    "The third column is just the product of the previous two. The sum of this column, 270, is the normalizing constant. To get the last column, which contains the posteriors, we divide the third column by the normalizing constant.\n",
    "\n",
    "That’s it. Simple, right?\n",
    "\n",
    "\n",
    "Well, you might be bothered by one detail. I write p(D|H) in terms of percentages, not probabilities, which means it is off by a factor of 10,000. But that cancels out when we divide through by the normalizing constant, so it doesn’t affect the result.\n",
    "\n",
    "When the set of hypotheses is mutually exclusive and collectively exhaustive, you can multiply the likelihoods by any factor, if it is convenient, as long as you apply the same factor to the entire column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
